{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Tensorflow Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question a\n",
    "Implement the softmax function using TensorFlow in q1_softmax.py. Remember that\n",
    "\n",
    "$$\n",
    "softmax(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "Note that you may not use tf.nn.softmax or related built-in functions. You can run basic (non-exhaustive) tests by running q1_softmax.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function in tensorflow.\n",
    "\n",
    "    You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "    tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "    not need to use all of these functions). Recall also that many common\n",
    "    tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "    if x and y are both tensors). Make sure to implement the numerical stability\n",
    "    fixes as in the previous homework!\n",
    "\n",
    "    Args:\n",
    "    x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "         represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "         input as in the previous homework)\n",
    "    Returns:\n",
    "    out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "         tensor in this problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    m = tf.reduce_max(x, reduction_indices=[1]) # Take the max of each row\n",
    "    size = np.array(x.eval()).shape[0] # Take the number of rows\n",
    "    m = tf.reshape(m, (size, 1)) # Reshape the vector as a column\n",
    "    x -= m # substract the maxes to x\n",
    "    \n",
    "    numerator = tf.exp(x) # compute the exp\n",
    "    denominator = tf.reduce_sum(numerator, 1) # compute the sum of exp\n",
    "    out = numerator / denominator # Compute the softmax\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "  \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print(\"Running basic tests...\")\n",
    "    test1 = softmax(tf.convert_to_tensor(\n",
    "      np.array([[1001,1002],[3,4]]), dtype=tf.float32))\n",
    "    with tf.Session():\n",
    "        test1 = test1.eval()\n",
    "    assert np.amax(np.fabs(test1 - np.array(\n",
    "      [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "    test2 = softmax(tf.convert_to_tensor(\n",
    "      np.array([[-1001,-1002]]), dtype=tf.float32))\n",
    "    with tf.Session():\n",
    "        test2 = test2.eval()\n",
    "    assert np.amax(np.fabs(test2 - np.array(\n",
    "      [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "    print(\"Basic (non-exhaustive) softmax tests pass\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question b\n",
    "Implement the cross-entropy loss using TensorFlow in q1_softmax.py. Remember that: \n",
    "\n",
    "$$\n",
    "CE(y, \\hat y) = - \\sum_{i = 1}^{N_c}y_ilog(\\hat {y_i})\n",
    "$$\n",
    "\n",
    "where $y \\in R^5$ is a one-hot vector and $N_c$ is the number of classes. Note that you may not use TensorFlow's built-in cross entropy functions for this question. You can run basic (non-exhaustive) tests by running python q1_softmax.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, yhat):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss in tensorflow.\n",
    "\n",
    "    y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "    be of dtype tf.float32.\n",
    "\n",
    "    The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "    solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "    Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "        functions.\n",
    "\n",
    "    Args:\n",
    "    y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "    yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "          probability distribution and should sum to 1.\n",
    "    Returns:\n",
    "    out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "          tensor in the problem.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    y = tf.to_float(y)\n",
    "    out = -tf.reduce_sum(y * tf.log(yhat))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our cross entropy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_cross_entropy_loss_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "    yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "    test1 = cross_entropy_loss(\n",
    "      tf.convert_to_tensor(y, dtype=tf.int32),\n",
    "      tf.convert_to_tensor(yhat, dtype=tf.float32))\n",
    "    with tf.Session():\n",
    "        test1 = test1.eval()\n",
    "    result = -3 * np.log(.5)\n",
    "    assert np.amax(np.fabs(test1 - result)) <= 1e-6\n",
    "    print(\"Basic (non-exhaustive) cross-entropy tests pass\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic (non-exhaustive) cross-entropy tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cross_entropy_loss_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question c\n",
    "Carefully study the Model class in model.py. Briefly explain the purpose of placeholder variables and feed dictionnaries in TensorFlow computations. Fill in the implementations for the add_placeholders, create_feed_dict in q1_classifier.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders are the input nodes, but they are not containing any data. We use the dictionnary feed to feed them with values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from q1_softmax import softmax\n",
    "from q1_softmax import cross_entropy_loss\n",
    "from model import Model\n",
    "from utils import data_iterator\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    batch_size = 64\n",
    "    n_samples = 1024\n",
    "    n_features = 100\n",
    "    n_classes = 5\n",
    "    # You may adjust the max_epochs to ensure convergence.\n",
    "    max_epochs = 50\n",
    "    # You may adjust this learning rate to ensure convergence.\n",
    "    lr = 1e-4 \n",
    "\n",
    "class SoftmaxModel(Model):\n",
    "    \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Creates a synthetic dataset and stores it in memory.\"\"\"\n",
    "        np.random.seed(1234)\n",
    "        self.input_data = np.random.rand(\n",
    "            self.config.n_samples, self.config.n_features)\n",
    "        self.input_labels = np.ones((self.config.n_samples,), dtype=np.int32)\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        code and will be fed data during training.\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of shape\n",
    "                           (batch_size, n_features), type tf.float32\n",
    "        labels_placeholder: Labels placeholder tensor of shape\n",
    "                           (batch_size, n_classes), type tf.int32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "\n",
    "          self.input_placeholder\n",
    "          self.labels_placeholder\n",
    "\n",
    "        (Don't change the variable names)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.input_placeholder = tf.placeholder(tf.float32, \n",
    "                                                shape=(self.config.batch_size,\n",
    "                                                self.config.n_features))\n",
    "        self.labels_placeholder = tf.placeholder(tf.int32, shape=(self.config.batch_size,\n",
    "                                                                 self.config.n_classes))\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, input_batch, label_batch):\n",
    "        \"\"\"Creates the feed_dict for softmax classifier.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "\n",
    "        feed_dict = {\n",
    "            <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "            ....\n",
    "        }\n",
    "\n",
    "        If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should match the placeholder tensors\n",
    "              created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "          input_batch: A batch of input data.\n",
    "          label_batch: A batch of label data.\n",
    "        Returns:\n",
    "          feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        feed_dict = {self.input_placeholder: input_batch,\n",
    "                    self.labels_placeholder: label_batch}\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See \n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "              Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "          loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "          train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def add_model(self, input_data):\n",
    "        \"\"\"Adds a linear-layer plus a softmax transformation\n",
    "\n",
    "        The core transformation for this model which transforms a batch of input\n",
    "        data into a batch of predictions. In this case, the mathematical\n",
    "        transformation effected is\n",
    "\n",
    "        y = softmax(xW + b)\n",
    "\n",
    "        Hint: Make sure to create tf.Variables as needed. Also, make sure to use\n",
    "              tf.name_scope to ensure that your name spaces are clean.\n",
    "        Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "              and biases b with zeros.\n",
    "\n",
    "        Args:\n",
    "          input_data: A tensor of shape (batch_size, n_features).\n",
    "        Returns:\n",
    "          out: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "        ### END YOUR CODE\n",
    "        return out\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "        Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "              short function.\n",
    "        Args:\n",
    "          pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "          loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def run_epoch(self, sess, input_data, input_labels):\n",
    "        \"\"\"Runs an epoch of training.\n",
    "\n",
    "        Trains the model for one-epoch.\n",
    "\n",
    "        Args:\n",
    "          sess: tf.Session() object\n",
    "          input_data: np.ndarray of shape (n_samples, n_features)\n",
    "          input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "          average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "        \"\"\"\n",
    "        # And then after everything is built, start the training loop.\n",
    "        average_loss = 0\n",
    "        for step, (input_batch, label_batch) in enumerate(\n",
    "            data_iterator(input_data, input_labels,\n",
    "                          batch_size=self.config.batch_size,\n",
    "                          label_size=self.config.n_classes)):\n",
    "\n",
    "          # Fill a feed dictionary with the actual set of images and labels\n",
    "          # for this particular training step.\n",
    "          feed_dict = self.create_feed_dict(input_batch, label_batch)\n",
    "\n",
    "          # Run one step of the model.  The return values are the activations\n",
    "          # from the `self.train_op` (which is discarded) and the `loss` Op.  To\n",
    "          # inspect the values of your Ops or variables, you may include them\n",
    "          # in the list passed to sess.run() and the value tensors will be\n",
    "          # returned in the tuple from the call.\n",
    "          _, loss_value = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "          average_loss += loss_value\n",
    "\n",
    "        average_loss = average_loss / step\n",
    "        return average_loss \n",
    "\n",
    "    def fit(self, sess, input_data, input_labels):\n",
    "        \"\"\"Fit model on provided data.\n",
    "\n",
    "        Args:\n",
    "          sess: tf.Session()\n",
    "          input_data: np.ndarray of shape (n_samples, n_features)\n",
    "          input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "          losses: list of loss per epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(self.config.max_epochs):\n",
    "          start_time = time.time()\n",
    "          average_loss = self.run_epoch(sess, input_data, input_labels)\n",
    "          duration = time.time() - start_time\n",
    "          # Print status to stdout.\n",
    "          print('Epoch %d: loss = %.2f (%.3f sec)'\n",
    "                 % (epoch, average_loss, duration))\n",
    "          losses.append(average_loss)\n",
    "        return losses\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the model.\n",
    "\n",
    "        Args:\n",
    "          config: A model configuration object of type Config\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        # Generate placeholders for the images and labels.\n",
    "        self.load_data()\n",
    "        self.add_placeholders()\n",
    "        self.pred = self.add_model(self.input_placeholder)\n",
    "        self.loss = self.add_loss_op(self.pred)\n",
    "        self.train_op = self.add_training_op(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question d\n",
    "Implement the transformation for a softmax classifier in function add_model in q1_classifier.py. Add cross-entropy loss in function add_loss_op in the same file. Use the implementations from the earlier parts of the problem, not TensorFlow built-ins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from q1_softmax import softmax\n",
    "from q1_softmax import cross_entropy_loss\n",
    "from model import Model\n",
    "from utils import data_iterator\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    batch_size = 64\n",
    "    n_samples = 1024\n",
    "    n_features = 100\n",
    "    n_classes = 5\n",
    "    # You may adjust the max_epochs to ensure convergence.\n",
    "    max_epochs = 50\n",
    "    # You may adjust this learning rate to ensure convergence.\n",
    "    lr = 1e-4 \n",
    "\n",
    "class SoftmaxModel(Model):\n",
    "    \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Creates a synthetic dataset and stores it in memory.\"\"\"\n",
    "        np.random.seed(1234)\n",
    "        self.input_data = np.random.rand(\n",
    "            self.config.n_samples, self.config.n_features)\n",
    "        self.input_labels = np.ones((self.config.n_samples,), dtype=np.int32)\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        code and will be fed data during training.\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of shape\n",
    "                           (batch_size, n_features), type tf.float32\n",
    "        labels_placeholder: Labels placeholder tensor of shape\n",
    "                           (batch_size, n_classes), type tf.int32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "\n",
    "          self.input_placeholder\n",
    "          self.labels_placeholder\n",
    "\n",
    "        (Don't change the variable names)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.input_placeholder = tf.placeholder(tf.float32, \n",
    "                                                shape=(self.config.batch_size,\n",
    "                                                self.config.n_features))\n",
    "        self.labels_placeholder = tf.placeholder(tf.int32, shape=(self.config.batch_size,\n",
    "                                                                 self.config.n_classes))\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, input_batch, label_batch):\n",
    "        \"\"\"Creates the feed_dict for softmax classifier.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "\n",
    "        feed_dict = {\n",
    "            <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "            ....\n",
    "        }\n",
    "\n",
    "        If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should match the placeholder tensors\n",
    "              created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "          input_batch: A batch of input data.\n",
    "          label_batch: A batch of label data.\n",
    "        Returns:\n",
    "          feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        feed_dict = {self.input_placeholder: input_batch,\n",
    "                    self.labels_placeholder: label_batch}\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See \n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "              Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "          loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "          train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def add_model(self, input_data):\n",
    "        \"\"\"Adds a linear-layer plus a softmax transformation\n",
    "\n",
    "        The core transformation for this model which transforms a batch of input\n",
    "        data into a batch of predictions. In this case, the mathematical\n",
    "        transformation effected is\n",
    "\n",
    "        y = softmax(xW + b)\n",
    "\n",
    "        Hint: Make sure to create tf.Variables as needed. Also, make sure to use\n",
    "              tf.name_scope to ensure that your name spaces are clean.\n",
    "        Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "              and biases b with zeros.\n",
    "\n",
    "        Args:\n",
    "          input_data: A tensor of shape (batch_size, n_features).\n",
    "        Returns:\n",
    "          out: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        n_features = self.config.n_features\n",
    "        n_classes = self.config.n_classes\n",
    "        \n",
    "        with tf.name_scope('softmax'):\n",
    "            weights = tf.Variable(tf.zeros([n_features, n_classes]), name = \"weights\")\n",
    "            biases = tf.Variable(tf.zeros([n_classes]), name = 'biases')\n",
    "            \n",
    "            linear = tf.matmul(input_data, weights) + biases\n",
    "            out = softmax(linear)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return out\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "        Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "              short function.\n",
    "        Args:\n",
    "          pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "          loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        loss = cross_entropy_loss(self.labels_placeholder, pred)\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def run_epoch(self, sess, input_data, input_labels):\n",
    "        \"\"\"Runs an epoch of training.\n",
    "\n",
    "        Trains the model for one-epoch.\n",
    "\n",
    "        Args:\n",
    "          sess: tf.Session() object\n",
    "          input_data: np.ndarray of shape (n_samples, n_features)\n",
    "          input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "          average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "        \"\"\"\n",
    "        # And then after everything is built, start the training loop.\n",
    "        average_loss = 0\n",
    "        for step, (input_batch, label_batch) in enumerate(\n",
    "            data_iterator(input_data, input_labels,\n",
    "                          batch_size=self.config.batch_size,\n",
    "                          label_size=self.config.n_classes)):\n",
    "\n",
    "          # Fill a feed dictionary with the actual set of images and labels\n",
    "          # for this particular training step.\n",
    "          feed_dict = self.create_feed_dict(input_batch, label_batch)\n",
    "\n",
    "          # Run one step of the model.  The return values are the activations\n",
    "          # from the `self.train_op` (which is discarded) and the `loss` Op.  To\n",
    "          # inspect the values of your Ops or variables, you may include them\n",
    "          # in the list passed to sess.run() and the value tensors will be\n",
    "          # returned in the tuple from the call.\n",
    "          _, loss_value = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "          average_loss += loss_value\n",
    "\n",
    "        average_loss = average_loss / step\n",
    "        return average_loss \n",
    "\n",
    "    def fit(self, sess, input_data, input_labels):\n",
    "        \"\"\"Fit model on provided data.\n",
    "\n",
    "        Args:\n",
    "          sess: tf.Session()\n",
    "          input_data: np.ndarray of shape (n_samples, n_features)\n",
    "          input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "          losses: list of loss per epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(self.config.max_epochs):\n",
    "          start_time = time.time()\n",
    "          average_loss = self.run_epoch(sess, input_data, input_labels)\n",
    "          duration = time.time() - start_time\n",
    "          # Print status to stdout.\n",
    "          print('Epoch %d: loss = %.2f (%.3f sec)'\n",
    "                 % (epoch, average_loss, duration))\n",
    "          losses.append(average_loss)\n",
    "        return losses\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the model.\n",
    "\n",
    "        Args:\n",
    "          config: A model configuration object of type Config\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        # Generate placeholders for the images and labels.\n",
    "        self.load_data()\n",
    "        self.add_placeholders()\n",
    "        self.pred = self.add_model(self.input_placeholder)\n",
    "        self.loss = self.add_loss_op(self.pred)\n",
    "        self.train_op = self.add_training_op(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question e\n",
    "Fill in the implementation for add_training_op in q1_classifier.py. Explain how TensorFlow's automatic differentiation removes the need to define gradients explicitely. Verify that your model is able to fit synyjetic data by running python q1_classifier.py and making sure that the test pass.\n",
    "\n",
    "HInt: Make sure to use the learning rate specified in Config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from q1_softmax import softmax\n",
    "from q1_softmax import cross_entropy_loss\n",
    "from model import Model\n",
    "from utils import data_iterator\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    batch_size = 64\n",
    "    n_samples = 1024\n",
    "    n_features = 100\n",
    "    n_classes = 5\n",
    "    # You may adjust the max_epochs to ensure convergence.\n",
    "    max_epochs = 50\n",
    "    # You may adjust this learning rate to ensure convergence.\n",
    "    lr = 1e-4 \n",
    "\n",
    "class SoftmaxModel(Model):\n",
    "    \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Creates a synthetic dataset and stores it in memory.\"\"\"\n",
    "        np.random.seed(1234)\n",
    "        self.input_data = np.random.rand(\n",
    "            self.config.n_samples, self.config.n_features)\n",
    "        self.input_labels = np.ones((self.config.n_samples,), dtype=np.int32)\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        code and will be fed data during training.\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of shape\n",
    "                           (batch_size, n_features), type tf.float32\n",
    "        labels_placeholder: Labels placeholder tensor of shape\n",
    "                           (batch_size, n_classes), type tf.int32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "\n",
    "          self.input_placeholder\n",
    "          self.labels_placeholder\n",
    "\n",
    "        (Don't change the variable names)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.input_placeholder = tf.placeholder(tf.float32, \n",
    "                                                shape=(self.config.batch_size,\n",
    "                                                self.config.n_features))\n",
    "        self.labels_placeholder = tf.placeholder(tf.int32, shape=(self.config.batch_size,\n",
    "                                                                 self.config.n_classes))\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, input_batch, label_batch):\n",
    "        \"\"\"Creates the feed_dict for softmax classifier.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "\n",
    "        feed_dict = {\n",
    "            <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "            ....\n",
    "        }\n",
    "\n",
    "        If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should match the placeholder tensors\n",
    "              created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "          input_batch: A batch of input data.\n",
    "          label_batch: A batch of label data.\n",
    "        Returns:\n",
    "          feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        feed_dict = {self.input_placeholder: input_batch,\n",
    "                    self.labels_placeholder: label_batch}\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See \n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "              Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "          loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "          train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.config.lr)\n",
    "        step = tf.Variable(0, name = 'step', trainable = False)\n",
    "        train_op = optimizer.minimize(loss, global_step = step)\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def add_model(self, input_data):\n",
    "        \"\"\"Adds a linear-layer plus a softmax transformation\n",
    "\n",
    "        The core transformation for this model which transforms a batch of input\n",
    "        data into a batch of predictions. In this case, the mathematical\n",
    "        transformation effected is\n",
    "\n",
    "        y = softmax(xW + b)\n",
    "\n",
    "        Hint: Make sure to create tf.Variables as needed. Also, make sure to use\n",
    "              tf.name_scope to ensure that your name spaces are clean.\n",
    "        Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "              and biases b with zeros.\n",
    "\n",
    "        Args:\n",
    "          input_data: A tensor of shape (batch_size, n_features).\n",
    "        Returns:\n",
    "          out: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        n_features = self.config.n_features\n",
    "        n_classes = self.config.n_classes\n",
    "        \n",
    "        with tf.name_scope('softmax'):\n",
    "            weights = tf.Variable(tf.zeros([n_features, n_classes]), name = \"weights\")\n",
    "            biases = tf.Variable(tf.zeros([n_classes]), name = 'biases')\n",
    "            \n",
    "            linear = tf.matmul(input_data, weights) + biases\n",
    "            out = softmax(linear)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return out\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "        Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "              short function.\n",
    "        Args:\n",
    "          pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "          loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        loss = cross_entropy_loss(self.labels_placeholder, pred)\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def run_epoch(self, sess, input_data, input_labels):\n",
    "        \"\"\"Runs an epoch of training.\n",
    "\n",
    "        Trains the model for one-epoch.\n",
    "\n",
    "        Args:\n",
    "          sess: tf.Session() object\n",
    "          input_data: np.ndarray of shape (n_samples, n_features)\n",
    "          input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "          average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "        \"\"\"\n",
    "        # And then after everything is built, start the training loop.\n",
    "        average_loss = 0\n",
    "        for step, (input_batch, label_batch) in enumerate(\n",
    "            data_iterator(input_data, input_labels,\n",
    "                          batch_size=self.config.batch_size,\n",
    "                          label_size=self.config.n_classes)):\n",
    "\n",
    "            # Fill a feed dictionary with the actual set of images and labels\n",
    "            # for this particular training step.\n",
    "            feed_dict = self.create_feed_dict(input_batch, label_batch)\n",
    "\n",
    "            # Run one step of the model.  The return values are the activations\n",
    "            # from the `self.train_op` (which is discarded) and the `loss` Op.  To\n",
    "            # inspect the values of your Ops or variables, you may include them\n",
    "            # in the list passed to sess.run() and the value tensors will be\n",
    "            # returned in the tuple from the call.\n",
    "            _, loss_value = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_value\n",
    "\n",
    "        average_loss = average_loss / step\n",
    "        return average_loss \n",
    "\n",
    "    def fit(self, sess, input_data, input_labels):\n",
    "        \"\"\"Fit model on provided data.\n",
    "\n",
    "        Args:\n",
    "          sess: tf.Session()\n",
    "          input_data: np.ndarray of shape (n_samples, n_features)\n",
    "          input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "          losses: list of loss per epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            start_time = time.time()\n",
    "            average_loss = self.run_epoch(sess, input_data, input_labels)\n",
    "            duration = time.time() - start_time\n",
    "            # Print status to stdout.\n",
    "            print('Epoch %d: loss = %.2f (%.3f sec)'\n",
    "                 % (epoch, average_loss, duration))\n",
    "            losses.append(average_loss)\n",
    "        return losses\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the model.\n",
    "\n",
    "        Args:\n",
    "          config: A model configuration object of type Config\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        # Generate placeholders for the images and labels.\n",
    "        self.load_data()\n",
    "        self.add_placeholders()\n",
    "        self.pred = self.add_model(self.input_placeholder)\n",
    "        self.loss = self.add_loss_op(self.pred)\n",
    "        self.train_op = self.add_training_op(self.loss)\n",
    "        \n",
    "def test_SoftmaxModel():\n",
    "    \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "    config = Config()\n",
    "    with tf.Graph().as_default():\n",
    "        model = SoftmaxModel(config)\n",
    "\n",
    "        # Create a session for running Ops on the Graph.\n",
    "        sess = tf.Session()\n",
    "\n",
    "        # Run the Op to initialize the variables.\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess.run(init)\n",
    "\n",
    "        losses = model.fit(sess, model.input_data, model.input_labels)\n",
    "\n",
    "    # If ops are implemented correctly, the average loss should fall close to zero\n",
    "    # rapidly.\n",
    "    assert losses[-1] < .5\n",
    "    print(\"Basic (non-exhaustive) classifier tests pass\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-6ebf66721cb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_SoftmaxModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-d60c1c426df1>\u001b[0m in \u001b[0;36mtest_SoftmaxModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSoftmaxModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[1;31m# Create a session for running Ops on the Graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-d60c1c426df1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_placeholders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_loss_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_training_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-d60c1c426df1>\u001b[0m in \u001b[0;36madd_model\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mlinear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[1;31m### END YOUR CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Cours\\Additional Courses\\Deep Learning for NLP\\Assignment2\\q1_softmax.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[1;31m### YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Take the max of each row\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Take the number of rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Reshape the vector as a column\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mm\u001b[0m \u001b[1;31m# substract the maxes to x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \"\"\"\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3622\u001b[0m                        \"`eval(session=sess)`\")\n\u001b[1;32m   3623\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3624\u001b[0;31m       raise ValueError(\"Cannot use the default session to evaluate tensor: \"\n\u001b[0m\u001b[1;32m   3625\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m                        \u001b[1;34m\"graph. Pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`."
     ]
    }
   ],
   "source": [
    "test_SoftmaxModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that TF automoatically compute the gradients, as soon as the graph is well defined!\n",
    "\n",
    "Now, let's test the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_SoftmaxModel():\n",
    "    \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "    config = Config()\n",
    "    with tf.Graph().as_default():\n",
    "        model = SoftmaxModel(config)\n",
    "\n",
    "        # Create a session for running Ops on the Graph.\n",
    "        sess = tf.Session()\n",
    "\n",
    "        # Run the Op to initialize the variables.\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess.run(init)\n",
    "\n",
    "        losses = model.fit(sess, model.input_data, model.input_labels)\n",
    "\n",
    "    # If ops are implemented correctly, the average loss should fall close to zero\n",
    "    # rapidly.\n",
    "    assert losses[-1] < .5\n",
    "    print(\"Basic (non-exhaustive) classifier tests pass\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-6ebf66721cb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_SoftmaxModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-160-cc4cbf176790>\u001b[0m in \u001b[0;36mtest_SoftmaxModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSoftmaxModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[1;31m# Create a session for running Ops on the Graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-20557fad3fc1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_placeholders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_loss_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_training_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-20557fad3fc1>\u001b[0m in \u001b[0;36madd_model\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mlinear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[1;31m### END YOUR CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Cours\\Additional Courses\\Deep Learning for NLP\\Assignment2\\q1_softmax.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[1;31m### YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Take the max of each row\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Take the number of rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Reshape the vector as a column\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mm\u001b[0m \u001b[1;31m# substract the maxes to x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \"\"\"\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3622\u001b[0m                        \"`eval(session=sess)`\")\n\u001b[1;32m   3623\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3624\u001b[0;31m       raise ValueError(\"Cannot use the default session to evaluate tensor: \"\n\u001b[0m\u001b[1;32m   3625\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m                        \u001b[1;34m\"graph. Pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`."
     ]
    }
   ],
   "source": [
    "test_SoftmaxModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_20' with dtype float and shape [64,100]\n\t [[Node: Placeholder_20 = Placeholder[dtype=DT_FLOAT, shape=[64,100], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'Placeholder_20', defined at:\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-163-bbd1341491b1>\", line 3, in <module>\n    model = SoftmaxModel(config)\n  File \"<ipython-input-151-20557fad3fc1>\", line 229, in __init__\n    self.add_placeholders()\n  File \"<ipython-input-151-20557fad3fc1>\", line 59, in add_placeholders\n    self.config.n_features))\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1512, in placeholder\n    name=name)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2043, in _placeholder\n    name=name)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_20' with dtype float and shape [64,100]\n\t [[Node: Placeholder_20 = Placeholder[dtype=DT_FLOAT, shape=[64,100], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_20' with dtype float and shape [64,100]\n\t [[Node: Placeholder_20 = Placeholder[dtype=DT_FLOAT, shape=[64,100], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-bbd1341491b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSoftmaxModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-151-20557fad3fc1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_placeholders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_loss_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_training_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-20557fad3fc1>\u001b[0m in \u001b[0;36madd_model\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mlinear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[1;31m### END YOUR CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Cours\\Additional Courses\\Deep Learning for NLP\\Assignment2\\q1_softmax.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[1;31m### YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Take the max of each row\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Take the number of rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Reshape the vector as a column\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mm\u001b[0m \u001b[1;31m# substract the maxes to x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \"\"\"\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3631\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3633\u001b[0;31m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_20' with dtype float and shape [64,100]\n\t [[Node: Placeholder_20 = Placeholder[dtype=DT_FLOAT, shape=[64,100], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'Placeholder_20', defined at:\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-163-bbd1341491b1>\", line 3, in <module>\n    model = SoftmaxModel(config)\n  File \"<ipython-input-151-20557fad3fc1>\", line 229, in __init__\n    self.add_placeholders()\n  File \"<ipython-input-151-20557fad3fc1>\", line 59, in add_placeholders\n    self.config.n_features))\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1512, in placeholder\n    name=name)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2043, in _placeholder\n    name=name)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_20' with dtype float and shape [64,100]\n\t [[Node: Placeholder_20 = Placeholder[dtype=DT_FLOAT, shape=[64,100], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "with tf.Session() as sess:\n",
    "    model = SoftmaxModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
